# Ollama Chat UI

A web-based user interface for interacting with Ollama models through a clean and simple chat interface. This application provides a seamless way to communicate with different Ollama models through a web browser.

## Features

- Clean web interface for chatting with Ollama models
- Model selection dropdown
- Real-time streaming responses
- Markdown support in responses
- Easy-to-use interface

## Prerequisites

- [Ollama](https://ollama.ai/) installed on your system

## Setup and Running

### Local Setup

1. First, start the Ollama server:
```bash
ollama serve
```

## Getting Started

First, run the development server:

```bash
npm run dev
```
# or

```bash
yarn dev
```
# or

```bash
pnpm dev
```
# or

```bash
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!



## Important Notes
- Make sure Ollama is running and accessible at http://localhost:11434 before starting the UI





